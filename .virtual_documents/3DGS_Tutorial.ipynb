











import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的模型
model = nn.Linear(2, 1)  # 输入 2，输出 1
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 前向传播和优化
x = torch.tensor([[1.0, 2.0]], requires_grad=True)
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()

# 查看 optimizer.state
for param in model.parameters():
    state = optimizer.state.get(param)
    print(f"Parameter: {param}")
    print(f"State: {state}")



